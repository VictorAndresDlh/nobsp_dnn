{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models import VGG11_Weights\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader as tf_dataloader\n",
    "from torch.utils.data import Subset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    #transforms.Resize(256),\n",
    "    #transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "# Train data\n",
    "trainset = torchvision.datasets.MNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "trainloader = tf_dataloader(\n",
    "    trainset, batch_size=batch_size, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "\n",
    "# Test data\n",
    "testset = torchvision.datasets.MNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "testloader = tf_dataloader(\n",
    "    testset, batch_size=batch_size, shuffle=False, num_workers=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    plt.figure(figsize=(2, 2))\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAK4AAACuCAYAAACvDDbuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAExElEQVR4nO3dPyitcRzH8edwFycWIp1NmSROCXHKbJNBSgxKMZsMdiVlkD8TGwuLQUbJvxiURZlMbBiQROcOt+7y/T6351yOcz7P836N336d83R73189z+84J5XP5/MBIKai1BcA/A/ChSTChSTChSTChSTChSTChSTChSTChaRfURemUqliXgcQBEEQRD3IZceFJMKFJMKFJMKFJMKFJMKFJMKFJMKFJMKFJMKFJMKFJMKFJMKFJMKFJMKFpMifx0W4dDrtznd2dsysv7/fXZvL5czs5OTkaxcWY+y4kES4kES4kES4kES4kMRThW9wdHTkzrPZrJmF/RXr0NCQmfFUIRw7LiQRLiQRLiQRLiSlov54CV/B9Ed7e7uZhd2czczMmJl3ExYEQZDJZMysra3NXfv29vavS5TGVzAh1ggXkggXkggXkggXkjjyLdDT05OZzc/Pu2uXl5fNrKWlxV3b19dnZlVVVe7aOD9ViIodF5IIF5IIF5IIF5I48v1he3t77tz769+6ujp37ePj47deUznhyBexRriQRLiQRLiQRLiQxJFvmbi6ujKzl5eXElyJBnZcSCJcSCJcSCJcSOLmrEzc3d2Z2fv7ewmuRAM7LiQRLiQRLiQRLiQRLiTxVKFM7O7ulvoSpLDjQhLhQhLhQhLhQhI3Z2Xi+fm51JcghR0XkggXkggXkggXkggXkniqUEQ1NTVm1tHR4a7d3Nws9uXECjsuJBEuJBEuJBEuJHFzVkTj4+Nm1tDQ4K4N+z1g+NhxIYlwIYlwIYlwIYlwISlRPxeVTqfd+dnZmZkdHBy4azc2Nszs8vLSXXt4eGhm9/f37trh4WF3njT8XBRijXAhiXAhiXAhKVFHvhMTE+68tbXVzGprayO/xsrKiru2qanJzJaWlv51iYiIHReSCBeSCBeSCBeSCBeSYnvkW1Fh/0+en5+7a72j4J6eHnft4uKimY2NjblrKysrzWx/f99dOzs7a2ZhR8lRj0UVceSLWCNcSCJcSCJcSIrtzVlzc7OZ3dzcuGu9Y9z19fXI7xX2ut41hPH+fefm5ty1r6+vZnZ7exv5vTKZjDtvbGw0s+np6civ+x24OUOsES4kES4kES4kES4kJeqD5A8PD+68kCcIg4ODZhb29KC7u9vMLi4u3LWjo6NmFvYl0Nls1sx6e3vdtYVYW1v78mv8FHZcSCJcSCJcSCJcSErUke/p6am7tr6+PvLrHh8fm5n3udsgCIJcLmdmn5+fkd8riTjyRawRLiQRLiQRLiQRLiQl6sg37O7f+yvfgYEBd21XV5eZdXZ2umt5glA87LiQRLiQRLiQRLiQFNubM+/GqLq62l27sLBgZlNTU+7ara0tM7u+vi7w6vBV7LiQRLiQRLiQRLiQRLiQFNsPknsmJyfd+erqqpltb2+7a0dGRszs4+PjaxeGv/ggOWKNcCGJcCGJcCEpUTdnKH/cnCHWCBeSCBeSCBeSCBeSCBeSCBeSCBeSCBeSCBeSCBeSCBeSCBeSCBeSCBeSCBeSCBeSCBeSCBeSCBeSCBeSCBeSCBeSCBeSCBeSCBeSCBeSIv9cVNTvdAJ+AjsuJBEuJBEuJBEuJBEuJBEuJBEuJBEuJBEuJP0Gq4LokLH6EZQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "print(labels)\n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAK4AAACuCAYAAACvDDbuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAEsklEQVR4nO3dwSt0URjH8TO8ysJCU5QFKTsboSQR2VCW/AuyYaNY2tta+AuslFJKSqIskCwUkppZICmLWSCSGsu33vNc7x137ri/O9/P8um8nMX3PXXn4GaKxWLRAWJqfnsDwE8QLiQRLiQRLiQRLiQRLiQRLiQRLiQRLiT9Cbswk8nEuQ/AOedc2ItcTlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxICv3WnSSYmpryZtPT0+bah4cHb/b+/m6uXVtb82aPj4/m2lwu990WUSGcuJBEuJBEuJBEuJBEuJCUKYZ8eWoS3uWbz+e9WXt7eyzf6/n52ZxfXl7G8v3icn9/782Wl5fNtWdnZ3Fv5794ly9SjXAhiXAhiXAhSerK17re7erqMtdeXV15s87OTnNtd3e3NxsZGTHX9vf3e7O7uztzbWtrqzkP6/Pz05w/PT15s5aWltBf9/b21pwn4eEsLE5cSCJcSCJcSCJcSCJcSJK68q2kxsZGc97T0+PNTk9PzbV9fX2R9vD29mbOb25uvNn19bW5NpvNerPZ2Vlz7erqagm7iwdXvkg1woUkwoUkwoUkHs4ETU5OerP19XVz7cXFhTcLus4uFAqR9lUOPJwh1QgXkggXkggXkggXkvhUIcGamprMufVJQXNzs7nW+ntrGxsb0TYWIz5VQKoRLiQRLiQRLiRJ/ZZvtQn6uVnroS3oujbo53TVceJCEuFCEuFCEuFCEuFCEle+CTEwMODN9vf3zbV1dXXebHh42Fx7eHgYbWMVxpUvUo1wIYlwIYlwIYkr34SYmJjwZtZDmHPO7e3tebOjo6Oy7ynJOHEhiXAhiXAhiXAhiXAhiU8VKqy+vt6cj4+Pe7OPjw9z7dLSkjcLerVUWnHiQhLhQhLhQhLhQhIPZxW2uLhozq33Ce/s7Jhrq+1618KJC0mEC0mEC0mEC0mEC0n8lm+MrB8O39zcNNe+vr56M+sa2Dnnjo+PI+0ryfgtX6Qa4UIS4UIS4UISV75lkM1mzfnKyoo3q62tNddub297szQ/hEXFiQtJhAtJhAtJhAtJhAtJXPmWqKbG/79+cnJiru3t7fVmuVzOXDs2NubN8vl8ibvTx5UvUo1wIYlwIYlwIYkr3xJ1dHR4M+shLMj8/Lw5r8YHsSg4cSGJcCGJcCGJcCGJcCGJTxUCtLW1mfPd3d3QX2NhYcGbbW1t/XhP+IsTF5IIF5IIF5IIF5J4OAswMzNjzoMe2iwHBwdl2g3+xYkLSYQLSYQLSYQLSYQLSXyq4JwbHBz0ZnNzc7+wE4TFiQtJhAtJhAtJhAtJPJw554aGhrxZQ0ND6H8f9GeVXl5efrwnfI8TF5IIF5IIF5IIF5IIF5L4VKFE5+fn3mx0dNRcWygU4t5O1eLEhSTChSTChSTChSTeuoNE4a07SDXChSTChSTChSTChaTQV75hn/aASuDEhSTChSTChSTChSTChSTChSTChSTChSTChaQvhSTqo5O4bu4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "print(labels)\n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer1 = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3, stride=2, padding=0)\n",
    "        \n",
    "        self.layer2 = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, stride=1, padding=0)\n",
    "        \n",
    "        self.layer3 = nn.Flatten()\n",
    "        self.drop1 = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=968, out_features=512)\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=10)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        self.x1 = F.relu(self.layer1(x))\n",
    "        self.x2 = F.relu(self.layer2(self.x1))\n",
    "        \n",
    "        self.x3 = self.layer3(self.x2)\n",
    "        self.x4 = self.drop1(self.x3)\n",
    "        \n",
    "        self.x5 = F.relu(self.fc1(self.x4))\n",
    "        self.x6 = F.softmax(self.fc2(self.x5), dim = 1)\n",
    "            \n",
    "        return self.x3, self.x6\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss at step 0: 2.302\n",
      "Train loss at step 10000: 1.653\n",
      "Train loss at step 20000: 1.605\n",
      "Train loss at step 30000: 1.586\n",
      "Train loss at step 40000: 1.573\n",
      "Train loss at step 50000: 1.566\n",
      "-----------------------------------\n",
      "Epoch 1\n",
      "Train Loss: 1.561\n",
      "Test Loss: 1.519\n",
      "Train Accuracy: 90.018%\n",
      "Test Accuracy: 94.170%\n",
      "-----------------------------------\n",
      "Train loss at step 0: 1.461\n",
      "Train loss at step 10000: 1.534\n",
      "Train loss at step 20000: 1.531\n",
      "Train loss at step 30000: 1.528\n",
      "Train loss at step 40000: 1.526\n",
      "Train loss at step 50000: 1.527\n",
      "-----------------------------------\n",
      "Epoch 2\n",
      "Train Loss: 1.528\n",
      "Test Loss: 1.516\n",
      "Train Accuracy: 93.272%\n",
      "Test Accuracy: 94.480%\n",
      "-----------------------------------\n",
      "Train loss at step 0: 1.461\n",
      "Train loss at step 10000: 1.524\n",
      "Train loss at step 20000: 1.522\n",
      "Train loss at step 30000: 1.524\n",
      "Train loss at step 40000: 1.522\n",
      "Train loss at step 50000: 1.524\n",
      "-----------------------------------\n",
      "Epoch 3\n",
      "Train Loss: 1.523\n",
      "Test Loss: 1.515\n",
      "Train Accuracy: 93.762%\n",
      "Test Accuracy: 94.650%\n",
      "-----------------------------------\n",
      "Train loss at step 0: 1.461\n",
      "Train loss at step 10000: 1.517\n",
      "Train loss at step 20000: 1.518\n",
      "Train loss at step 30000: 1.523\n",
      "Train loss at step 40000: 1.522\n",
      "Train loss at step 50000: 1.521\n",
      "-----------------------------------\n",
      "Epoch 4\n",
      "Train Loss: 1.522\n",
      "Test Loss: 1.516\n",
      "Train Accuracy: 93.893%\n",
      "Test Accuracy: 94.500%\n",
      "-----------------------------------\n",
      "Train loss at step 0: 1.461\n",
      "Train loss at step 10000: 1.520\n",
      "Train loss at step 20000: 1.517\n",
      "Train loss at step 30000: 1.517\n",
      "Train loss at step 40000: 1.517\n",
      "Train loss at step 50000: 1.518\n",
      "-----------------------------------\n",
      "Epoch 5\n",
      "Train Loss: 1.517\n",
      "Test Loss: 1.516\n",
      "Train Accuracy: 94.408%\n",
      "Test Accuracy: 94.460%\n",
      "-----------------------------------\n",
      "Early stopping at epoch 5\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "patience = 2  # Define the number of epochs to tolerate before early stopping\n",
    "best_loss = float('inf')\n",
    "counter = 0\n",
    "\n",
    "y = []\n",
    "train_images = []\n",
    "tensors = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    test_loss = 0.0\n",
    "    train_correct = 0\n",
    "    test_correct = 0\n",
    "    \n",
    "    # Train \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        tensor, outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # print train_loss each time i is multiple of 1000\n",
    "        if i % 10000 == 0:\n",
    "            print(f'Train loss at step {i}: {train_loss/(i+1):.3f}')\n",
    "        \n",
    "        # If we are in the last epoch or the patience has been reached, save all batches of images and labels\n",
    "        if epoch == epochs - 1 or counter == patience - 1:\n",
    "            train_images.append(inputs)\n",
    "            y.append(labels)\n",
    "            tensors.append(tensor)\n",
    "    \n",
    "    \n",
    "    net.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            _, outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "    \n",
    "    # Print test loss for the epoch\n",
    "    print(\"-----------------------------------\")\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    print(f\"Train Loss: {train_loss / len(trainloader):.3f}\")\n",
    "    print(f\"Test Loss: {test_loss / len(testloader):.3f}\")\n",
    "    print(f'Train Accuracy: {100 * train_correct / len(trainloader):.3f}%')\n",
    "    print(f'Test Accuracy: {100 * test_correct / len(testloader):.3f}%')\n",
    "    print(\"-----------------------------------\")\n",
    "    \n",
    "\n",
    "    # Check for early stopping\n",
    "    if test_loss < best_loss:\n",
    "        best_loss = test_loss\n",
    "        counter = 0\n",
    "        # Save the best model\n",
    "        torch.save(net.state_dict(), 'best_model_mnist.pth')\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "    \n",
    "    # If we are not in the last epoch and the patience has not been reached, empty the lists\n",
    "    if epoch != epochs - 1 and counter != patience - 1:\n",
    "        train_images = []\n",
    "        y = []\n",
    "        tensors = []\n",
    "    \n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 94.650 %\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the trained model on test data\n",
    "net.load_state_dict(torch.load('best_model_mnist.pth'))\n",
    "net.eval()  # Set model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(testloader, 0):\n",
    "        images, labels = data\n",
    "        _, outputs = net(images)\n",
    "        predicted = torch.argmax(outputs, dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {accuracy:.3f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(net.state_dict(), f\"cnn_trained_{epochs}_epch.pth\")\n",
    "torch.save(train_images, \"train_images_mnist.pth\")\n",
    "torch.save(tensors, \"train_tensors_mnist.pth\")\n",
    "torch.save(y, \"train_labels_mnist.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAK4AAACuCAYAAACvDDbuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAADoUlEQVR4nO3dMS40YRzH8Rk0onUHtEJHHIFKtS23ULqDxB3EGdhuEzoO4A5UaxVvOc8k4zWz/GY/n/KfKZ7wzRPP7I6pF4vFooIwa7+9APgfwiWScIkkXCIJl0jCJZJwiSRcIgmXSBtdL6zresh1QFVVVdX1g1w7LpGESyThEkm4RBIukYRLJOESSbhEEi6RhEsk4RJJuEQSLpGESyThEkm4RBIukYRLJOESSbhEEi6RhEsk4RJJuEQSLpGES6TO/4KJf2azWWP2/v5evHYymTRmb29vva9pFdlxiSRcIgmXSMIlknCJ5K5CD46Ojorzy8vLxuzq6mro5awEOy6RhEsk4RJJuERyOPum29vbxuzm5qZ47fb29tDLWVl2XCIJl0jCJZJwiSRcIrmr0IOu75+lP3ZcIgmXSMIlknCJVC86nizquh56LRF2d3cbs5eXl+K1pR/t+vp672sak64HXTsukYRLJOESSbhEEi6R3FXowXw+L85LP9rz8/PitXd3d72uKZW7CoyacIkkXCIJl0gOZz1oe8r34uKiMXt6eipee3h42OuaUjmcMWrCJZJwiSRcIgmXSJ7yHZCnf4djxyWScIkkXCIJl0gOZz2YTqfFeekj362treK1m5ubjdnHx8fPFjZidlwiCZdIwiWScIkkXCL5InkPSncEqqqqZrNZY7a3t1e89uDgoDF7fn7+2cIC+SI5oyZcIgmXSMIlko98e9D20Wxp3nbIPTk5acxW8XDWlR2XSMIlknCJJFwiCZdI7ioM6PX1tTHb398vXruzszP0ckbFjksk4RJJuEQSLpEczgZUevp3Mpn8wkrGx45LJOESSbhEEi6RhEskdxWWrO0p1ranfymz4xJJuEQSLpGESySHsyVre8r3+Ph4ySvJZsclknCJJFwiCZdIwiWSuwpL1vaRr/f+fo8dl0jCJZJwiSRcIjmcDejh4aExW1sr7xWfn59DL2dU7LhEEi6RhEsk4RJJuETyLt8lm8/nxXnp17CxsXo3fbzLl1ETLpGESyThEmn1/vr/Zff398X52dlZY9b25O/j42OPK8pkxyWScIkkXCIJl0jCJZK7Ckt2fX1dnJ+enjZmbf/s2V0FOy6hhEsk4RJJuETyfVz+FN/HZdSESyThEkm4RBIukYRLJOESSbhEEi6RhEsk4RJJuEQSLpGESyThEkm4ROr8lK93zfKX2HGJJFwiCZdIwiWScIkkXCIJl0jCJZJwifQFCTCdunNnpXMAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get some random training images\n",
    "#dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "print(labels)\n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, pred = net(images)\n",
    "torch.argmax(pred, dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
