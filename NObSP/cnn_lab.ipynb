{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader as tf_dataloader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    ")\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "trainloader = tf_dataloader(\n",
    "    trainset, batch_size=batch_size, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "testset = torchvision.datasets.MNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "testloader = tf_dataloader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = tuple(str(i) for i in range(10))  # MNIST classes are digits from 0 to 9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4    \n"
     ]
    }
   ],
   "source": [
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(\" \".join(f\"{classes[labels[j]]:5s}\" for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.layer2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.layer3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.layer4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.layer5 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.layer6 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.layer7 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.layer8 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.layer9 = nn.Flatten()\n",
    "        self.drop1 = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.drop2 = nn.Dropout(p=0.2)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.drop3 = nn.Dropout(p=0.2)\n",
    "        self.fc3 = nn.Linear(in_features=64, out_features=10) \n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x1 = self.layer1(x)\n",
    "        self.x2 = F.relu(self.layer2(self.x1))\n",
    "        self.x3 = self.layer3(self.x2)\n",
    "        self.x4 = F.relu(self.layer4(self.x3))\n",
    "        self.x5 = self.layer5(self.x4)\n",
    "        self.x6 = F.relu(self.layer6(self.x5))\n",
    "        self.x7 = self.layer7(self.x6)\n",
    "        self.x8 = F.relu(self.layer8(self.x7))\n",
    "        self.x9 = self.layer9(self.x8)\n",
    "        self.x10 = self.drop1(self.x9)\n",
    "        \n",
    "        self.x11 = F.relu(self.fc1(self.x10))\n",
    "        self.x12 = self.drop2(self.x11)\n",
    "        self.x13 = F.relu(self.fc2(self.x12))\n",
    "        self.x14 = self.drop3(self.x13)\n",
    "        self.x15 = F.softmax(self.fc3(self.x14), dim = 1)\n",
    "            \n",
    "        return self.x15\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x16a11afd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hook(module, input, output):\n",
    "    output_tensors.append(output)\n",
    "\n",
    "net.fc2.register_forward_hook(hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  1000] loss: 1.145\n",
      "[1,  2000] loss: 1.067\n",
      "[1,  3000] loss: 0.976\n",
      "[1,  4000] loss: 0.937\n",
      "[1,  5000] loss: 0.899\n",
      "[1,  6000] loss: 0.866\n",
      "[1,  7000] loss: 0.840\n",
      "[1,  8000] loss: 0.830\n",
      "[1,  9000] loss: 0.812\n",
      "[1, 10000] loss: 0.808\n",
      "[1, 11000] loss: 0.795\n",
      "[1, 12000] loss: 0.784\n",
      "[1, 13000] loss: 0.794\n",
      "[1, 14000] loss: 0.783\n",
      "[1, 15000] loss: 0.782\n",
      "[1, 16000] loss: 0.783\n",
      "[1, 17000] loss: 0.773\n",
      "[1, 18000] loss: 0.770\n",
      "[1, 19000] loss: 0.773\n",
      "[1, 20000] loss: 0.773\n",
      "[1, 21000] loss: 0.762\n",
      "[1, 22000] loss: 0.765\n",
      "[1, 23000] loss: 0.774\n",
      "[1, 24000] loss: 0.762\n",
      "[1, 25000] loss: 0.772\n",
      "[1, 26000] loss: 0.768\n",
      "[1, 27000] loss: 0.762\n",
      "[1, 28000] loss: 0.764\n",
      "[1, 29000] loss: 0.764\n",
      "[1, 30000] loss: 0.763\n",
      "[1, 31000] loss: 0.754\n",
      "[1, 32000] loss: 0.766\n",
      "[1, 33000] loss: 0.765\n",
      "[1, 34000] loss: 0.754\n",
      "[1, 35000] loss: 0.760\n",
      "[1, 36000] loss: 0.764\n",
      "[1, 37000] loss: 0.752\n",
      "[1, 38000] loss: 0.754\n",
      "[1, 39000] loss: 0.752\n",
      "[1, 40000] loss: 0.757\n",
      "[1, 41000] loss: 0.752\n",
      "[1, 42000] loss: 0.762\n",
      "[1, 43000] loss: 0.750\n",
      "[1, 44000] loss: 0.754\n",
      "[1, 45000] loss: 0.754\n",
      "[1, 46000] loss: 0.750\n",
      "[1, 47000] loss: 0.753\n",
      "[1, 48000] loss: 0.755\n",
      "[1, 49000] loss: 0.756\n",
      "[1, 50000] loss: 0.754\n",
      "[1, 51000] loss: 0.751\n",
      "[1, 52000] loss: 0.753\n",
      "[1, 53000] loss: 0.754\n",
      "[1, 54000] loss: 0.750\n",
      "[1, 55000] loss: 0.752\n",
      "[1, 56000] loss: 0.746\n",
      "[1, 57000] loss: 0.750\n",
      "[1, 58000] loss: 0.756\n",
      "[1, 59000] loss: 0.749\n",
      "[1, 60000] loss: 0.752\n",
      "Test Loss: 1.492647094452381\n",
      "[2,  1000] loss: 0.753\n",
      "[2,  2000] loss: 0.747\n",
      "[2,  3000] loss: 0.751\n",
      "[2,  4000] loss: 0.753\n",
      "[2,  5000] loss: 0.750\n",
      "[2,  6000] loss: 0.746\n",
      "[2,  7000] loss: 0.747\n",
      "[2,  8000] loss: 0.746\n",
      "[2,  9000] loss: 0.745\n",
      "[2, 10000] loss: 0.746\n",
      "[2, 11000] loss: 0.748\n",
      "[2, 12000] loss: 0.754\n",
      "[2, 13000] loss: 0.752\n",
      "[2, 14000] loss: 0.746\n",
      "[2, 15000] loss: 0.747\n",
      "[2, 16000] loss: 0.749\n",
      "[2, 17000] loss: 0.744\n",
      "[2, 18000] loss: 0.744\n",
      "[2, 19000] loss: 0.742\n",
      "[2, 20000] loss: 0.751\n",
      "[2, 21000] loss: 0.748\n",
      "[2, 22000] loss: 0.746\n",
      "[2, 23000] loss: 0.747\n",
      "[2, 24000] loss: 0.745\n",
      "[2, 25000] loss: 0.750\n",
      "[2, 26000] loss: 0.745\n",
      "[2, 27000] loss: 0.747\n",
      "[2, 28000] loss: 0.750\n",
      "[2, 29000] loss: 0.748\n",
      "[2, 30000] loss: 0.751\n",
      "[2, 31000] loss: 0.749\n",
      "[2, 32000] loss: 0.748\n",
      "[2, 33000] loss: 0.749\n",
      "[2, 34000] loss: 0.745\n",
      "[2, 35000] loss: 0.742\n",
      "[2, 36000] loss: 0.744\n",
      "[2, 37000] loss: 0.745\n",
      "[2, 38000] loss: 0.751\n",
      "[2, 39000] loss: 0.750\n",
      "[2, 40000] loss: 0.748\n",
      "[2, 41000] loss: 0.747\n",
      "[2, 42000] loss: 0.745\n",
      "[2, 43000] loss: 0.748\n",
      "[2, 44000] loss: 0.744\n",
      "[2, 45000] loss: 0.747\n",
      "[2, 46000] loss: 0.747\n",
      "[2, 47000] loss: 0.744\n",
      "[2, 48000] loss: 0.745\n",
      "[2, 49000] loss: 0.743\n",
      "[2, 50000] loss: 0.743\n",
      "[2, 51000] loss: 0.743\n",
      "[2, 52000] loss: 0.754\n",
      "[2, 53000] loss: 0.751\n",
      "[2, 54000] loss: 0.741\n",
      "[2, 55000] loss: 0.743\n",
      "[2, 56000] loss: 0.752\n",
      "[2, 57000] loss: 0.749\n",
      "[2, 58000] loss: 0.744\n",
      "[2, 59000] loss: 0.749\n",
      "[2, 60000] loss: 0.742\n",
      "Test Loss: 1.4930210605740548\n",
      "[3,  1000] loss: 0.742\n",
      "[3,  2000] loss: 0.743\n",
      "[3,  3000] loss: 0.743\n",
      "[3,  4000] loss: 0.747\n",
      "[3,  5000] loss: 0.746\n",
      "[3,  6000] loss: 0.744\n",
      "[3,  7000] loss: 0.745\n",
      "[3,  8000] loss: 0.742\n",
      "[3,  9000] loss: 0.744\n",
      "[3, 10000] loss: 0.742\n",
      "[3, 11000] loss: 0.744\n",
      "[3, 12000] loss: 0.745\n",
      "[3, 13000] loss: 0.743\n",
      "[3, 14000] loss: 0.742\n",
      "[3, 15000] loss: 0.742\n",
      "[3, 16000] loss: 0.748\n",
      "[3, 17000] loss: 0.746\n",
      "[3, 18000] loss: 0.744\n",
      "[3, 19000] loss: 0.745\n",
      "[3, 20000] loss: 0.748\n",
      "[3, 21000] loss: 0.740\n",
      "[3, 22000] loss: 0.741\n",
      "[3, 23000] loss: 0.751\n",
      "[3, 24000] loss: 0.743\n",
      "[3, 25000] loss: 0.741\n",
      "[3, 26000] loss: 0.743\n",
      "[3, 27000] loss: 0.740\n",
      "[3, 28000] loss: 0.743\n",
      "[3, 29000] loss: 0.742\n",
      "[3, 30000] loss: 0.741\n",
      "[3, 31000] loss: 0.745\n",
      "[3, 32000] loss: 0.741\n",
      "[3, 33000] loss: 0.743\n",
      "[3, 34000] loss: 0.746\n",
      "[3, 35000] loss: 0.746\n",
      "[3, 36000] loss: 0.743\n",
      "[3, 37000] loss: 0.747\n",
      "[3, 38000] loss: 0.744\n",
      "[3, 39000] loss: 0.749\n",
      "[3, 40000] loss: 0.740\n",
      "[3, 41000] loss: 0.748\n",
      "[3, 42000] loss: 0.745\n",
      "[3, 43000] loss: 0.741\n",
      "[3, 44000] loss: 0.742\n",
      "[3, 45000] loss: 0.745\n",
      "[3, 46000] loss: 0.743\n",
      "[3, 47000] loss: 0.744\n",
      "[3, 48000] loss: 0.740\n",
      "[3, 49000] loss: 0.744\n",
      "[3, 50000] loss: 0.747\n",
      "[3, 51000] loss: 0.738\n",
      "[3, 52000] loss: 0.748\n",
      "[3, 53000] loss: 0.747\n",
      "[3, 54000] loss: 0.747\n",
      "[3, 55000] loss: 0.742\n",
      "[3, 56000] loss: 0.747\n",
      "[3, 57000] loss: 0.742\n",
      "[3, 58000] loss: 0.744\n",
      "[3, 59000] loss: 0.746\n",
      "[3, 60000] loss: 0.746\n",
      "Test Loss: 1.4985789774775504\n",
      "[4,  1000] loss: 0.743\n",
      "[4,  2000] loss: 0.743\n",
      "[4,  3000] loss: 0.739\n",
      "[4,  4000] loss: 0.743\n",
      "[4,  5000] loss: 0.744\n",
      "[4,  6000] loss: 0.747\n",
      "[4,  7000] loss: 0.740\n",
      "[4,  8000] loss: 0.741\n",
      "[4,  9000] loss: 0.746\n",
      "[4, 10000] loss: 0.743\n",
      "[4, 11000] loss: 0.744\n",
      "[4, 12000] loss: 0.744\n",
      "[4, 13000] loss: 0.745\n",
      "[4, 14000] loss: 0.746\n",
      "[4, 15000] loss: 0.741\n",
      "[4, 16000] loss: 0.744\n",
      "[4, 17000] loss: 0.741\n",
      "[4, 18000] loss: 0.748\n",
      "[4, 19000] loss: 0.740\n",
      "[4, 20000] loss: 0.738\n",
      "[4, 21000] loss: 0.746\n",
      "[4, 22000] loss: 0.748\n",
      "[4, 23000] loss: 0.742\n",
      "[4, 24000] loss: 0.739\n",
      "[4, 25000] loss: 0.740\n",
      "[4, 26000] loss: 0.738\n",
      "[4, 27000] loss: 0.741\n",
      "[4, 28000] loss: 0.742\n",
      "[4, 29000] loss: 0.739\n",
      "[4, 30000] loss: 0.747\n",
      "[4, 31000] loss: 0.739\n",
      "[4, 32000] loss: 0.744\n",
      "[4, 33000] loss: 0.746\n",
      "[4, 34000] loss: 0.748\n",
      "[4, 35000] loss: 0.739\n",
      "[4, 36000] loss: 0.737\n",
      "[4, 37000] loss: 0.739\n",
      "[4, 38000] loss: 0.737\n",
      "[4, 39000] loss: 0.748\n",
      "[4, 40000] loss: 0.738\n",
      "[4, 41000] loss: 0.744\n",
      "[4, 42000] loss: 0.740\n",
      "[4, 43000] loss: 0.742\n",
      "[4, 44000] loss: 0.739\n",
      "[4, 45000] loss: 0.745\n",
      "[4, 46000] loss: 0.746\n",
      "[4, 47000] loss: 0.737\n",
      "[4, 48000] loss: 0.741\n",
      "[4, 49000] loss: 0.744\n",
      "[4, 50000] loss: 0.741\n",
      "[4, 51000] loss: 0.740\n",
      "[4, 52000] loss: 0.746\n",
      "[4, 53000] loss: 0.741\n",
      "[4, 54000] loss: 0.743\n",
      "[4, 55000] loss: 0.746\n",
      "[4, 56000] loss: 0.739\n",
      "[4, 57000] loss: 0.738\n",
      "[4, 58000] loss: 0.740\n",
      "[4, 59000] loss: 0.740\n",
      "[4, 60000] loss: 0.744\n",
      "Test Loss: 1.4829307265996934\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "epochs = 4\n",
    "patience = 5  # Define the number of epochs to tolerate before early stopping\n",
    "best_loss = float('inf')\n",
    "counter = 0\n",
    "\n",
    "y = []\n",
    "images = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    output_tensors = []\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        \n",
    "        if epoch == epochs-1:\n",
    "            images.append(inputs)\n",
    "            y.append(labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:  # print every 1000 mini-batches\n",
    "            print(f\"[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}\")\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    # Calculate test loss\n",
    "    test_loss = 0.0\n",
    "    net.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "    \n",
    "    # Print test loss for the epoch\n",
    "    print(f\"Test Loss: {test_loss / len(testloader)}\")\n",
    "    \n",
    "    # Check for early stopping\n",
    "    if test_loss < best_loss:\n",
    "        best_loss = test_loss\n",
    "        counter = 0\n",
    "        # Save the best model\n",
    "        torch.save(net.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            images.append(inputs)\n",
    "            y.append(labels)\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "    \n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 97.81 %\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the trained model on test data\n",
    "net.load_state_dict(torch.load('best_model.pth'))\n",
    "net.eval()  # Set model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(testloader, 0):\n",
    "        images, labels = data\n",
    "\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {accuracy:.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(net.state_dict(), f\"cnn_trained_{epochs}_epch.pth\")\n",
    "torch.save(images, \"train_images_MNIST.pth\")\n",
    "torch.save(output_tensors, \"train_tensors_MNIST.pth\")\n",
    "torch.save(y, \"train_labels_MNIST.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'model.pth')\n",
    "net2 = torch.load('model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['layer1.weight', 'layer1.bias', 'layer3.weight', 'layer3.bias', 'layer5.weight', 'layer5.bias', 'layer7.weight', 'layer7.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(net2).keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
