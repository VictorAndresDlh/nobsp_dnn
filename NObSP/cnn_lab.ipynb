{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader as tf_dataloader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    ")\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "trainloader = tf_dataloader(\n",
    "    trainset, batch_size=batch_size, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "testset = torchvision.datasets.MNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "testloader = tf_dataloader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = tuple(str(i) for i in range(10))  # MNIST classes are digits from 0 to 9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7    \n"
     ]
    }
   ],
   "source": [
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(\" \".join(f\"{classes[labels[j]]:5s}\" for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n",
    "        self.layer2 = nn.MaxPool2d(kernel_size=3, stride=3)\n",
    "        self.layer3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        self.layer4 = nn.MaxPool2d(kernel_size=3, stride=3)\n",
    "        self.layer5 = nn.Flatten()\n",
    "        self.drop1 = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.drop2 = nn.Dropout(p=0.2)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x1 = self.layer1(x)\n",
    "        self.x2 = F.relu(self.layer2(self.x1))\n",
    "        self.x3 = self.layer3(self.x2)\n",
    "        self.x4 = F.relu(self.layer4(self.x3))\n",
    "        self.x5 = self.layer5(self.x4)\n",
    "        self.x6 = self.drop1(self.x5)\n",
    "        \n",
    "        self.x7 = F.relu(self.fc1(self.x6))\n",
    "        self.x8 = self.drop2(self.x7)\n",
    "        self.x9 = F.softmax(self.fc2(self.x8), dim = 1)\n",
    "            \n",
    "        return self.x9\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.layer2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.layer3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.layer4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.layer5 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.layer6 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.layer7 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.layer8 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.layer9 = nn.Flatten()\n",
    "        self.drop1 = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.drop2 = nn.Dropout(p=0.2)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.drop3 = nn.Dropout(p=0.2)\n",
    "        self.fc3 = nn.Linear(in_features=64, out_features=10) \n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x1 = self.layer1(x)\n",
    "        self.x2 = F.relu(self.layer2(self.x1))\n",
    "        self.x3 = self.layer3(self.x2)\n",
    "        self.x4 = F.relu(self.layer4(self.x3))\n",
    "        self.x5 = self.layer5(self.x4)\n",
    "        self.x6 = F.relu(self.layer6(self.x5))\n",
    "        self.x7 = self.layer7(self.x6)\n",
    "        self.x8 = F.relu(self.layer8(self.x7))\n",
    "        self.x9 = self.layer9(self.x8)\n",
    "        self.x10 = self.drop1(self.x9)\n",
    "        \n",
    "        self.x11 = F.relu(self.fc1(self.x10))\n",
    "        self.x12 = self.drop2(self.x11)\n",
    "        self.x13 = F.relu(self.fc2(self.x12))\n",
    "        self.x14 = self.drop3(self.x13)\n",
    "        self.x15 = F.softmax(self.fc3(self.x14), dim = 1)\n",
    "            \n",
    "        return self.x15\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x17c0a1210>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hook(module, input, output):\n",
    "    output_tensors.append(output)\n",
    "\n",
    "net.fc1.register_forward_hook(hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  1000] loss: 1.138\n",
      "[1,  2000] loss: 1.045\n",
      "[1,  3000] loss: 0.962\n",
      "[1,  4000] loss: 0.911\n",
      "[1,  5000] loss: 0.862\n",
      "[1,  6000] loss: 0.823\n",
      "[1,  7000] loss: 0.825\n",
      "[1,  8000] loss: 0.803\n",
      "[1,  9000] loss: 0.793\n",
      "[1, 10000] loss: 0.793\n",
      "[1, 11000] loss: 0.787\n",
      "[1, 12000] loss: 0.782\n",
      "[1, 13000] loss: 0.782\n",
      "[1, 14000] loss: 0.783\n",
      "[1, 15000] loss: 0.782\n",
      "[1, 16000] loss: 0.774\n",
      "[1, 17000] loss: 0.776\n",
      "[1, 18000] loss: 0.774\n",
      "[1, 19000] loss: 0.772\n",
      "[1, 20000] loss: 0.768\n",
      "[1, 21000] loss: 0.767\n",
      "[1, 22000] loss: 0.767\n",
      "[1, 23000] loss: 0.771\n",
      "[1, 24000] loss: 0.765\n",
      "[1, 25000] loss: 0.769\n",
      "[1, 26000] loss: 0.761\n",
      "[1, 27000] loss: 0.765\n",
      "[1, 28000] loss: 0.763\n",
      "[1, 29000] loss: 0.766\n",
      "[1, 30000] loss: 0.762\n",
      "[1, 31000] loss: 0.762\n",
      "[1, 32000] loss: 0.765\n",
      "[1, 33000] loss: 0.759\n",
      "[1, 34000] loss: 0.760\n",
      "[1, 35000] loss: 0.758\n",
      "[1, 36000] loss: 0.755\n",
      "[1, 37000] loss: 0.760\n",
      "[1, 38000] loss: 0.758\n",
      "[1, 39000] loss: 0.762\n",
      "[1, 40000] loss: 0.759\n",
      "[1, 41000] loss: 0.758\n",
      "[1, 42000] loss: 0.755\n",
      "[1, 43000] loss: 0.753\n",
      "[1, 44000] loss: 0.756\n",
      "[1, 45000] loss: 0.753\n",
      "[1, 46000] loss: 0.752\n",
      "[1, 47000] loss: 0.756\n",
      "[1, 48000] loss: 0.757\n",
      "[1, 49000] loss: 0.753\n",
      "[1, 50000] loss: 0.750\n",
      "[1, 51000] loss: 0.751\n",
      "[1, 52000] loss: 0.752\n",
      "[1, 53000] loss: 0.753\n",
      "[1, 54000] loss: 0.756\n",
      "[1, 55000] loss: 0.753\n",
      "[1, 56000] loss: 0.751\n",
      "[1, 57000] loss: 0.751\n",
      "[1, 58000] loss: 0.753\n",
      "[1, 59000] loss: 0.754\n",
      "[1, 60000] loss: 0.753\n",
      "Test Loss: 1.4866249790549277\n",
      "[2,  1000] loss: 0.745\n",
      "[2,  2000] loss: 0.744\n",
      "[2,  3000] loss: 0.742\n",
      "[2,  4000] loss: 0.744\n",
      "[2,  5000] loss: 0.745\n",
      "[2,  6000] loss: 0.747\n",
      "[2,  7000] loss: 0.742\n",
      "[2,  8000] loss: 0.746\n",
      "[2,  9000] loss: 0.745\n",
      "[2, 10000] loss: 0.743\n",
      "[2, 11000] loss: 0.744\n",
      "[2, 12000] loss: 0.746\n",
      "[2, 13000] loss: 0.746\n",
      "[2, 14000] loss: 0.741\n",
      "[2, 15000] loss: 0.744\n",
      "[2, 16000] loss: 0.744\n",
      "[2, 17000] loss: 0.743\n",
      "[2, 18000] loss: 0.741\n",
      "[2, 19000] loss: 0.745\n",
      "[2, 20000] loss: 0.742\n",
      "[2, 21000] loss: 0.746\n",
      "[2, 22000] loss: 0.746\n",
      "[2, 23000] loss: 0.744\n",
      "[2, 24000] loss: 0.744\n",
      "[2, 25000] loss: 0.742\n",
      "[2, 26000] loss: 0.745\n",
      "[2, 27000] loss: 0.741\n",
      "[2, 28000] loss: 0.740\n",
      "[2, 29000] loss: 0.741\n",
      "[2, 30000] loss: 0.744\n",
      "[2, 31000] loss: 0.739\n",
      "[2, 32000] loss: 0.745\n",
      "[2, 33000] loss: 0.745\n",
      "[2, 34000] loss: 0.742\n",
      "[2, 35000] loss: 0.740\n",
      "[2, 36000] loss: 0.740\n",
      "[2, 37000] loss: 0.747\n",
      "[2, 38000] loss: 0.740\n",
      "[2, 39000] loss: 0.739\n",
      "[2, 40000] loss: 0.743\n",
      "[2, 41000] loss: 0.745\n",
      "[2, 42000] loss: 0.746\n",
      "[2, 43000] loss: 0.742\n",
      "[2, 44000] loss: 0.742\n",
      "[2, 45000] loss: 0.743\n",
      "[2, 46000] loss: 0.741\n",
      "[2, 47000] loss: 0.743\n",
      "[2, 48000] loss: 0.740\n",
      "[2, 49000] loss: 0.744\n",
      "[2, 50000] loss: 0.738\n",
      "[2, 51000] loss: 0.739\n",
      "[2, 52000] loss: 0.744\n",
      "[2, 53000] loss: 0.745\n",
      "[2, 54000] loss: 0.745\n",
      "[2, 55000] loss: 0.741\n",
      "[2, 56000] loss: 0.742\n",
      "[2, 57000] loss: 0.741\n",
      "[2, 58000] loss: 0.741\n",
      "[2, 59000] loss: 0.743\n",
      "[2, 60000] loss: 0.742\n",
      "Test Loss: 1.4844836194753648\n",
      "[3,  1000] loss: 0.740\n",
      "[3,  2000] loss: 0.739\n",
      "[3,  3000] loss: 0.743\n",
      "[3,  4000] loss: 0.745\n",
      "[3,  5000] loss: 0.742\n",
      "[3,  6000] loss: 0.734\n",
      "[3,  7000] loss: 0.742\n",
      "[3,  8000] loss: 0.739\n",
      "[3,  9000] loss: 0.740\n",
      "[3, 10000] loss: 0.741\n",
      "[3, 11000] loss: 0.741\n",
      "[3, 12000] loss: 0.739\n",
      "[3, 13000] loss: 0.740\n",
      "[3, 14000] loss: 0.738\n",
      "[3, 15000] loss: 0.738\n",
      "[3, 16000] loss: 0.743\n",
      "[3, 17000] loss: 0.739\n",
      "[3, 18000] loss: 0.742\n",
      "[3, 19000] loss: 0.740\n",
      "[3, 20000] loss: 0.740\n",
      "[3, 21000] loss: 0.738\n",
      "[3, 22000] loss: 0.739\n",
      "[3, 23000] loss: 0.738\n",
      "[3, 24000] loss: 0.743\n",
      "[3, 25000] loss: 0.742\n",
      "[3, 26000] loss: 0.741\n",
      "[3, 27000] loss: 0.741\n",
      "[3, 28000] loss: 0.737\n",
      "[3, 29000] loss: 0.740\n",
      "[3, 30000] loss: 0.743\n",
      "[3, 31000] loss: 0.738\n",
      "[3, 32000] loss: 0.738\n",
      "[3, 33000] loss: 0.740\n",
      "[3, 34000] loss: 0.739\n",
      "[3, 35000] loss: 0.739\n",
      "[3, 36000] loss: 0.739\n",
      "[3, 37000] loss: 0.739\n",
      "[3, 38000] loss: 0.739\n",
      "[3, 39000] loss: 0.742\n",
      "[3, 40000] loss: 0.735\n",
      "[3, 41000] loss: 0.742\n",
      "[3, 42000] loss: 0.739\n",
      "[3, 43000] loss: 0.742\n",
      "[3, 44000] loss: 0.742\n",
      "[3, 45000] loss: 0.738\n",
      "[3, 46000] loss: 0.737\n",
      "[3, 47000] loss: 0.740\n",
      "[3, 48000] loss: 0.739\n",
      "[3, 49000] loss: 0.740\n",
      "[3, 50000] loss: 0.737\n",
      "[3, 51000] loss: 0.738\n",
      "[3, 52000] loss: 0.741\n",
      "[3, 53000] loss: 0.736\n",
      "[3, 54000] loss: 0.738\n",
      "[3, 55000] loss: 0.744\n",
      "[3, 56000] loss: 0.738\n",
      "[3, 57000] loss: 0.742\n",
      "[3, 58000] loss: 0.743\n",
      "[3, 59000] loss: 0.744\n",
      "[3, 60000] loss: 0.739\n",
      "Test Loss: 1.4777266756772995\n",
      "[4,  1000] loss: 0.738\n",
      "[4,  2000] loss: 0.737\n",
      "[4,  3000] loss: 0.740\n",
      "[4,  4000] loss: 0.737\n",
      "[4,  5000] loss: 0.737\n",
      "[4,  6000] loss: 0.742\n",
      "[4,  7000] loss: 0.738\n",
      "[4,  8000] loss: 0.738\n",
      "[4,  9000] loss: 0.740\n",
      "[4, 10000] loss: 0.742\n",
      "[4, 11000] loss: 0.738\n",
      "[4, 12000] loss: 0.740\n",
      "[4, 13000] loss: 0.738\n",
      "[4, 14000] loss: 0.740\n",
      "[4, 15000] loss: 0.739\n",
      "[4, 16000] loss: 0.738\n",
      "[4, 17000] loss: 0.737\n",
      "[4, 18000] loss: 0.739\n",
      "[4, 19000] loss: 0.736\n",
      "[4, 20000] loss: 0.738\n",
      "[4, 21000] loss: 0.738\n",
      "[4, 22000] loss: 0.739\n",
      "[4, 23000] loss: 0.743\n",
      "[4, 24000] loss: 0.741\n",
      "[4, 25000] loss: 0.739\n",
      "[4, 26000] loss: 0.738\n",
      "[4, 27000] loss: 0.736\n",
      "[4, 28000] loss: 0.737\n",
      "[4, 29000] loss: 0.738\n",
      "[4, 30000] loss: 0.734\n",
      "[4, 31000] loss: 0.735\n",
      "[4, 32000] loss: 0.739\n",
      "[4, 33000] loss: 0.737\n",
      "[4, 34000] loss: 0.742\n",
      "[4, 35000] loss: 0.738\n",
      "[4, 36000] loss: 0.736\n",
      "[4, 37000] loss: 0.739\n",
      "[4, 38000] loss: 0.740\n",
      "[4, 39000] loss: 0.739\n",
      "[4, 40000] loss: 0.737\n",
      "[4, 41000] loss: 0.736\n",
      "[4, 42000] loss: 0.739\n",
      "[4, 43000] loss: 0.734\n",
      "[4, 44000] loss: 0.739\n",
      "[4, 45000] loss: 0.738\n",
      "[4, 46000] loss: 0.739\n",
      "[4, 47000] loss: 0.738\n",
      "[4, 48000] loss: 0.740\n",
      "[4, 49000] loss: 0.739\n",
      "[4, 50000] loss: 0.739\n",
      "[4, 51000] loss: 0.738\n",
      "[4, 52000] loss: 0.738\n",
      "[4, 53000] loss: 0.738\n",
      "[4, 54000] loss: 0.739\n",
      "[4, 55000] loss: 0.734\n",
      "[4, 56000] loss: 0.739\n",
      "[4, 57000] loss: 0.739\n",
      "[4, 58000] loss: 0.739\n",
      "[4, 59000] loss: 0.740\n",
      "[4, 60000] loss: 0.739\n",
      "Test Loss: 1.4756896833539008\n",
      "[5,  1000] loss: 0.740\n",
      "[5,  2000] loss: 0.738\n",
      "[5,  3000] loss: 0.738\n",
      "[5,  4000] loss: 0.736\n",
      "[5,  5000] loss: 0.737\n",
      "[5,  6000] loss: 0.735\n",
      "[5,  7000] loss: 0.739\n",
      "[5,  8000] loss: 0.736\n",
      "[5,  9000] loss: 0.739\n",
      "[5, 10000] loss: 0.740\n",
      "[5, 11000] loss: 0.738\n",
      "[5, 12000] loss: 0.738\n",
      "[5, 13000] loss: 0.737\n",
      "[5, 14000] loss: 0.739\n",
      "[5, 15000] loss: 0.737\n",
      "[5, 16000] loss: 0.739\n",
      "[5, 17000] loss: 0.739\n",
      "[5, 18000] loss: 0.735\n",
      "[5, 19000] loss: 0.739\n",
      "[5, 20000] loss: 0.735\n",
      "[5, 21000] loss: 0.737\n",
      "[5, 22000] loss: 0.734\n",
      "[5, 23000] loss: 0.737\n",
      "[5, 24000] loss: 0.738\n",
      "[5, 25000] loss: 0.739\n",
      "[5, 26000] loss: 0.738\n",
      "[5, 27000] loss: 0.737\n",
      "[5, 28000] loss: 0.736\n",
      "[5, 29000] loss: 0.737\n",
      "[5, 30000] loss: 0.737\n",
      "[5, 31000] loss: 0.735\n",
      "[5, 32000] loss: 0.738\n",
      "[5, 33000] loss: 0.739\n",
      "[5, 34000] loss: 0.736\n",
      "[5, 35000] loss: 0.735\n",
      "[5, 36000] loss: 0.738\n",
      "[5, 37000] loss: 0.738\n",
      "[5, 38000] loss: 0.737\n",
      "[5, 39000] loss: 0.738\n",
      "[5, 40000] loss: 0.739\n",
      "[5, 41000] loss: 0.740\n",
      "[5, 42000] loss: 0.737\n",
      "[5, 43000] loss: 0.736\n",
      "[5, 44000] loss: 0.738\n",
      "[5, 45000] loss: 0.737\n",
      "[5, 46000] loss: 0.739\n",
      "[5, 47000] loss: 0.736\n",
      "[5, 48000] loss: 0.736\n",
      "[5, 49000] loss: 0.736\n",
      "[5, 50000] loss: 0.738\n",
      "[5, 51000] loss: 0.737\n",
      "[5, 52000] loss: 0.739\n",
      "[5, 53000] loss: 0.739\n",
      "[5, 54000] loss: 0.740\n",
      "[5, 55000] loss: 0.737\n",
      "[5, 56000] loss: 0.736\n",
      "[5, 57000] loss: 0.739\n",
      "[5, 58000] loss: 0.737\n",
      "[5, 59000] loss: 0.737\n",
      "[5, 60000] loss: 0.740\n",
      "Test Loss: 1.477815833723545\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "patience = 2  # Define the number of epochs to tolerate before early stopping\n",
    "best_loss = float('inf')\n",
    "counter = 0\n",
    "\n",
    "y = []\n",
    "images = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    output_tensors = []\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        \n",
    "        if epoch == epochs-1:\n",
    "            images.append(inputs)\n",
    "            y.append(labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:  # print every 1000 mini-batches\n",
    "            print(f\"[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}\")\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    # Calculate test loss\n",
    "    test_loss = 0.0\n",
    "    net.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "    \n",
    "    # Print test loss for the epoch\n",
    "    print(f\"Test Loss: {test_loss / len(testloader)}\")\n",
    "    \n",
    "    # Check for early stopping\n",
    "    if test_loss < best_loss:\n",
    "        best_loss = test_loss\n",
    "        counter = 0\n",
    "        # Save the best model\n",
    "        torch.save(net.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            images.append(inputs)\n",
    "            y.append(labels)\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "    \n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.64 %\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the trained model on test data\n",
    "net.load_state_dict(torch.load('best_model.pth'))\n",
    "net.eval()  # Set model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(testloader, 0):\n",
    "        images, labels = data\n",
    "\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {accuracy:.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(net.state_dict(), f\"cnn_trained_{epochs}_epch.pth\")\n",
    "torch.save(images, \"train_images_MNIST_simple.pth\")\n",
    "torch.save(output_tensors, \"train_tensors_MNIST_simple.pth\")\n",
    "torch.save(y, \"train_labels_MNIST_simple.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'model_MNIST_simple.pth')\n",
    "net2 = torch.load('model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
