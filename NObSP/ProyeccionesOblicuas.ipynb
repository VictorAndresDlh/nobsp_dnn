{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54df3c53-2cb3-495c-b8a6-5bfc558307ac",
   "metadata": {},
   "source": [
    "# Notas sobre Proyecciones Oblicuas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efa8c08-8864-440f-af76-e22d80bcb724",
   "metadata": {},
   "source": [
    "## Proyectores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f3a1d8-e0a3-49b7-bf4a-33805a78f109",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Definición:\n",
    "Sea $\\mathcal{V}\\subset\\mathbb{R}^{N}$, tal que $\\mathcal{V}=\\mathcal{X}\\oplus\\mathcal{Y}$ i.e. para cada $\\mathbf{v}\\in\\mathcal{V}$, existen $\\mathbf{x}\\in\\mathcal{X}$, $\\mathbf{y}\\in\\mathcal{Y}$ donde $\\mathbf{v} = \\mathbf{x} + \\mathbf{y}$. El operador lineal\n",
    "\n",
    "$$\n",
    "\\mathbf{P}:\\mathcal{V}\\rightarrow\\mathcal{V},\\\\\n",
    "\\mathbf{Pv}=\\mathbf{x}\n",
    "$$\n",
    "\n",
    "es el **proyector sobre $\\mathcal{X}$ a lo largo de $\\mathcal{Y}$**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd95c9a0-5c3e-44bd-9dc0-cf1bde8534f3",
   "metadata": {},
   "source": [
    "### Propiedades\n",
    "\n",
    "* **Idempotencia:** $\\mathbf{P}^2=\\mathbf{P}$. (*una vez aplicada la proyección, ya estoy viviendo en donde quería proyectar. Cualquier otra aplicación de la proyección me lleva a lo mismo*)\n",
    "\n",
    "* **Proyector complementario:** $\\mathbf{I}-\\mathbf{P}$ es el proyector sobre $\\mathcal{Y}$ a lo largo de $\\mathcal{X}$. (*basta cambiar la dirección para cambiar entre subespacio objetivo y subespacio referente*)\n",
    "\n",
    "* **Rango-Nulidad:** $R(\\mathbf{P}) = N(\\mathbf{I}-\\mathbf{P}) = \\mathcal{X}$ (*El rango de mi subespacio objetivo corresponde a la nulidad de mi proyección complementaria*)\n",
    "\n",
    "* **Clasificación:** Si $\\mathcal{X}\\perp\\mathcal{Y}$ se dice que $\\mathbf{P}$ es un ***proyector ortogonal***, en caso contrario ***proyector oblicuo***. Si bien la ortogonalidad facilita la construcción de un proyector en un espacio de Hilbert *(espacio vectorial con producto interior que induce una norma y es completo)* esto no siempre se tiene ni se puede garantizar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd8ed8a-6b85-458b-95b4-3283927b8307",
   "metadata": {},
   "source": [
    "### Construcción de Proyectores Oblicuos:\n",
    "Inicialmente desde [libro] se tiene que:\n",
    "$$\n",
    "\\mathbf{P} = [\\mathbf{X}|\\mathbf{0}][\\mathbf{X}|\\mathbf{Y}]^{-1}= [\\mathbf{X}|\\mathbf{Y}]\\begin{pmatrix}\n",
    "\\mathbf{I} & \\mathbf{0}\\\\\n",
    "\\mathbf{0} & \\mathbf{0}\n",
    "\\end{pmatrix}[\\mathbf{X}|\\mathbf{Y}]^{-1}\n",
    "$$\n",
    "donde $C(\\mathbf{X})=\\mathcal{X}$ y $C(\\mathbf{Y})=\\mathcal{Y}$.\n",
    "\n",
    "Ahora, en los desarrollos del profe Alexander:\n",
    "$$\n",
    "\\mathbf{P} = \\mathbf{X}^{T}\\left(\\mathbf{X}^{T}\\mathbf{Q}_{Y}\\mathbf{X}\\right)^{\\dagger}\\mathbf{X}^{T}\\mathbf{Q}_{Y}\n",
    "$$\n",
    "donde:\n",
    "$$\n",
    "\\mathbf{Q}_{Y} = \\mathbf{I} - \\mathbf{P}_{Y}\\\\\n",
    "\\mathbf{P}_{Y} = \\mathbf{Y}^{T}\\left(\\mathbf{Y}^{T}\\mathbf{Y}\\right)^{\\dagger}\\mathbf{Y}^{T}: \\text{Proyector Ortogonal sobre Y}\\\\\n",
    "\\dagger:=\\text{Pseudoinversa de Moore-Penrose}\n",
    "$$\n",
    "<span style=\"color:green\">REVISAR ALGORITMO</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695e5810-1550-4fb4-b1eb-b4275b2ceb85",
   "metadata": {},
   "source": [
    "### Uso en interpretabilidad de NN: Desde un enfoque de post-hoc XAI\n",
    "Los proyectores oblicuos permiten descomponer un output en las contribuciones parciales no lineales de algunos inputs. En este caso particular de redes neuronales, en el caso de feed forward, se tiene:\n",
    "\n",
    "#### Notación matricial:\n",
    "Considere una red neuronal de $l$ capas, ya entrenada, cuyo conjunto de observación es $\\{\\mathbf{x}^{(i)}, y^{(i)}\\}^{N}_{i=1}$. Para cada $j\\in \\{1, \\dots, l\\}$, la capa $j$ viene definida por $\\{\\mathbf{W}_{j}, \\mathbf{b}_{j}, f_{j}(\\cdot)\\}$ donde $\\mathbf{W}_{j}$ es la matriz de pesos de la capa, $\\mathbf{b}_{j}$, el vector de bias y $f_{j}(\\cdot)$ la función de activación. En ese sentido, el output de la última capa de la red se puede definir como:\n",
    "\n",
    "$$\n",
    "\\hat{y}^{(i)} = f_{l}\\left(\\mathbf{W}_{l}\\mathbf{z}^{(i)}+b_{l}\\right)\n",
    "$$\n",
    "\n",
    "donde $\\mathbf{z}\\in\\mathbb{R}^{p}$ es el output de la penúltima capa. Desde el punto de vista del espacio latente, (espacio de representación de la red neuronal):\n",
    "\n",
    "$$\n",
    "\\hat{y}_{\\text{latent}} = \\mathbf{W}_{l}\\mathbf{z}^{(i)}+b_{l}\n",
    "$$\n",
    "y sin pérdida de generalidad:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{y}}_{\\text{latent}}-b_{l} = \\mathbf{Z}\\mathbf{W}_{l}^{T}\n",
    "$$\n",
    "con $\\mathbf{Z}\\in\\mathbb{R}^{N\\times p}$ y $\\mathbf{Z} = \\left[\\left(\\mathbf{z}^{(1)}\\right)^{T};\\cdots;\\left(\\mathbf{z}^{(N)}\\right)^{T}\\right]$\n",
    "\n",
    "<span style=\"color:green\">1. ¿Z es el generado por los outputs de la penúltima capa de todas las observaciones?</span>. Sí\n",
    "\n",
    "#### En términos de las contribuciones:\n",
    "$$\n",
    "\\hat{y}_{\\text{latent}} = \\sum_{k=1}^{d}g_{k}(x_{k}) + \\sum_{k=1}^{d}\\sum_{m>k}^{d}g_{k,m}(x_{k}, x_{m}) + \\mathbf{G} + b_{l}\n",
    "$$\n",
    "donde:\n",
    "\n",
    "* $g_{k}(\\cdot)$: contribuciones no lineales en la salida de la $k$-ésima variable de entrada.\n",
    "\n",
    "* $g_{k, m}(\\cdot)$: efecto de interacción entre las $k$-ésima y $m$-ésima variables de entrada.\n",
    "\n",
    "* $\\mathbf{G}$: efectos de interacción de ordenes superiores\n",
    "\n",
    "\n",
    "#### Relación con el desarrollo previo:\n",
    "* $\\mathcal{V}:=C(\\mathbf{Z})$ (*en Z están embedidas las transformaciones no lineales que realiza la red neuronal sobre cada input*)\n",
    "\n",
    "* $\\hat{\\mathbf{y}}_{k} = g_{k}(\\mathbf{x}_{k})$: transformación no lineal del vector $\\mathbf{x}_{k}$ en el output, donde\n",
    "$$\n",
    "\\mathbf{x}_{k} = \\left[x_{k}^{(1)}, \\dots, x_{k}^{(N)}\\right]^{T}\n",
    "$$\n",
    "<span style=\"color:green\">2. ¿este vector x_k, es el vector con las k-ésimas entradas de todos los inputs?</span> Sí\n",
    "\n",
    "* $\\mathbf{Z} = f_{NN}(\\mathbf{X})$ donde $f_{NN}:\\mathbb{R}^{N\\times d}\\rightarrow \\mathbb{R}^{N\\times p}$ es la función que representa el mapeo desde el espacio de entrada hacia la penúltima capa de la red (*es lo que queremos desarmar*). En este caso:\n",
    "$$\n",
    "\\mathbf{X} = \\left[\\left(\\mathbf{x}^{(1)}\\right);\\dots;\\left(\\mathbf{x}^{(N)}\\right)\\right]^{T}\n",
    "$$\n",
    "<span style=\"color:green\">3. ¿X es entonces la matriz de todas las observaciones, de todas las variables de entrada?</span> Sí, son mis datos\n",
    "\n",
    "* $\\mathcal{V} = \\mathcal{V}_{k}\\oplus\\mathcal{V}_{(k)}$, donde:\n",
    "\n",
    "    * $\\mathcal{V}_{k} = C\\left(\\mathbf{Z}_{k}\\right)$\n",
    "    \n",
    "        * $\\mathbf{Z}_{k} = f_{NN}(\\mathbf{X}_{k})$\n",
    "    \n",
    "        * $\\mathbf{X}_{k} = \\left[\\left(\\mathbf{0}_{N}\\right)_{1}, \\dots ,\\left(\\mathbf{0}_{N}\\right)_{k-1}, \\mathbf{x}_{k}, \\left(\\mathbf{0}_{N}\\right)_{k+1}, \\dots, \\left(\\mathbf{0}_{N}\\right)_{d}\\right]$\n",
    "        \n",
    "        <span style=\"color:green\">4. ¿el subespacio objetivo, es el subespacio de las contribuciones del k-esimo input?</span>\n",
    "    \n",
    "    * $\\mathcal{V}_{(k)} = C\\left(\\mathbf{Z}_{(k)}\\right)$\n",
    "    \n",
    "        * $\\mathbf{Z}_{(k)} = f_{NN}(\\mathbf{X}_{(k)})$\n",
    "    \n",
    "        * $\\mathbf{X}_{(k)} = \\left[\\mathbf{x}_{1}, \\dots, \\mathbf{x}_{k-1}, \\left(\\mathbf{0}_{N}\\right)_{k}, \\mathbf{x}_{k+1}, \\dots, \\mathbf{x}_{d}\\right]$\n",
    "        \n",
    "        <span style=\"color:green\">5. ¿el subespacio referente, es el subespacio de las contribuciones de todos los demás inputs?</span>\n",
    "        \n",
    "<span style=\"color:green\">6. ¿Cómo se computa la función $f_{NN}$?</span> ES LA RED YA ENTRENADA\n",
    "        \n",
    "* Finalmente $\\hat{\\mathbf{y}} = \\mathbf{P}_{k/(k)}\\left(\\hat{\\mathbf{y}}_{\\text{latent}}-b_{l}\\right)$, donde:\n",
    "\n",
    "    * $\\mathbf{P}_{k/(k)} = \\mathbf{Z}_{k}\\left(\\mathbf{Z}_{k}^{T}\\mathbf{Q}_{(k)}\\mathbf{Z}_{k}\\right)^{\\dagger}\\mathbf{Z}_{k}^{T}\\mathbf{Q}_{(k)}$\n",
    "    \n",
    "        * $\\mathbf{Q}_{(k)} = \\mathbf{I}_{N}-\\mathbf{P}_{(k)}$ (ver definición)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f73ed1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a922a5-6c15-43c4-9c10-119d5dd2bc7c",
   "metadata": {},
   "source": [
    "### Opciones para *desarmar* las interacciones de segundo orden:\n",
    "\n",
    "#### 1. Epsilon\n",
    "Para el caso en que las interacciones individuales pueden verse contaminadas por las interacciones de segundo orden por la ejecución del algoritmo, se plantea el siguiente desarrollo:\n",
    "\n",
    "$$\n",
    "y_{i}(x_{i}, 0) = g_{i}(x_{i}) + \\sum_{m\\neq i}^{d}g_{i,m}(x_{i},0)\\\\\n",
    "y_{i}(x_{i}, \\epsilon) = g_{i}(x_{i}) + \\sum_{m\\neq i}^{d}g_{i,m}(x_{i},\\epsilon)\n",
    "$$\n",
    "Asumamos (1) *para $\\epsilon <<$*\n",
    "$$\n",
    "\\sum_{m\\neq i}^{d}g_{i,m}(x_{i},\\epsilon) = \\epsilon\\cdot\\sum_{m\\neq i}^{d}g_{i,m}(x_{i},0)\n",
    "$$\n",
    "Entonces:\n",
    "$$\n",
    "\\epsilon\\cdot y_{i}(x_{i}, 0) - y_{i}(x_{i}, \\epsilon) = \\epsilon\\cdot g_{i}(x_{i}) + \\epsilon\\cdot\\sum_{m\\neq i}^{d}g_{i,m}(x_{i},0) - g_{i}(x_{i}) -  \\sum_{m\\neq i}^{d}g_{i,m}(x_{i},\\epsilon)\n",
    "$$\n",
    "Luego, usando (1)\n",
    "$$\n",
    "\\epsilon\\cdot y_{i}(x_{i}, 0) - y_{i}(x_{i}, \\epsilon) = \\epsilon\\cdot g_{i}(x_{i}) + \\epsilon\\cdot\\sum_{m\\neq i}^{d}g_{i,m}(x_{i},0) - g_{i}(x_{i}) - \\epsilon\\cdot\\sum_{m\\neq i}^{d}g_{i,m}(x_{i},0)\\\\\n",
    "\\epsilon\\cdot y_{i}(x_{i}, 0) - y_{i}(x_{i}, \\epsilon) = (\\epsilon - 1) g_{i}(x_{i})\n",
    "$$\n",
    "Finalmente\n",
    "$$\n",
    "g_{i}(x_{i}) = \\dfrac{y_{i}(x_{i}, \\epsilon) - \\epsilon\\cdot y_{i}(x_{i}, 0)}{1-\\epsilon}\n",
    "$$\n",
    "Donde $y = P y_{est}$\n",
    "\n",
    "**Nota: Este desarrollo se implementa en `NObSP_Decomposition_eps.py` y se explora en el NB de NN**\n",
    "\n",
    "### 2. Series de Taylor\n",
    "#### 2.1 Desde 0s:\n",
    "Volvamos a la definición por contribuciones:\n",
    "$$\n",
    "\\hat{y}_{\\text{latent}} = \\sum_{k=1}^{d}g_{k}(x_{k}) + \\sum_{k=1}^{d}\\sum_{m>k}^{d}g_{k,m}(x_{k}, x_{m}) + \\mathbf{G} + b_{l}\n",
    "$$\n",
    "Queremos descomponer\n",
    "$$\n",
    "g_{k,m}(x_{k}, x_{m})\n",
    "$$\n",
    "Usando series de Taylor, tenemos que:\n",
    "$$\n",
    "g_{k,m}(x_{k}, x_{m}) \\approx g_{k,m}(0, 0) + \\dfrac{\\partial g_{k,m}(0, 0)}{\\partial x_{k}}x_{k} + \\dfrac{\\partial g_{k,m}(0, 0)}{\\partial x_{m}}x_{m} + \\dfrac{\\partial^{2} g_{k,m}(0, 0)}{2\\partial x_{k}^{2}}x_{k}^{2} + \\dfrac{\\partial^{2} g_{k,m}(0, 0)}{2\\partial x_{m}^{2}}x_{m}^{2} + \\dfrac{\\partial^{2} g_{k,m}(0, 0)}{\\partial x_{k}\\partial x_{m}}x_{k}x_{m}\n",
    "$$\n",
    "<span style=\"color:green\">En caso de pensarlo de esta forma, ¿hay alguna idea para computar esas derivadas parciales?</span>\n",
    "\n",
    "#### 2.2 Usando lo anterior:\n",
    "Podemos corregir la suposición que sea hace en (1):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a512f2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b6fc43-202f-4199-af14-14d1ef2a0957",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Material consultado:\n",
    "\n",
    "1. Meyer, C. D., & Stewart, I. (2023). Matrix analysis and applied linear algebra. Society for Industrial and Applied Mathematics.\n",
    "2. Caicedo, A., Varon, C., Van Huffel, S., & Suykens, J. A. (2019). Functional form estimation using oblique projection matrices for LS-SVM regression models. Plos one, 14(6), e0217967.\n",
    "3. R. T. Behrens and L. L. Scharf, \"Signal processing applications of oblique projection operators,\" in IEEE Transactions on Signal Processing, vol. 42, no. 6, pp. 1413-1424, June 1994, doi: 10.1109/78.286957.\n",
    "4. Caicedo, A., Varon, C., Hunyadi, B., Papademetriou, M., Tachtsidis, I., & Van Huffel, S. (2016). Decomposition of near-infrared spectroscopy signals using oblique subspace projections: applications in brain hemodynamic monitoring. Frontiers in physiology, 7, 515.\n",
    "5. MIT Open Course: https://www.youtube.com/watch?v=Y_Ac6KiQ1t0 y https://www.youtube.com/watch?v=osh80YCg_GM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c05f4bc-2428-4b5c-9aa7-e6b65d7154c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
